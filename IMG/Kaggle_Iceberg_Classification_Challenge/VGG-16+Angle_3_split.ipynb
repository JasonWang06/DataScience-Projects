{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "import os\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import rmsprop\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"/\".join(os.getcwd().split(\"/\")[:-1])+\"/Data/train.json\")\n",
    "test = pd.read_json(\"/\".join(os.getcwd().split(\"/\")[:-1])+\"/Data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train = train[train['inc_angle']!='na']\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "train.index = [i for i in range(len(train.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_band_test_3=(X_band_test_1+X_band_test_2)/2\n",
    "test1 = np.concatenate([X_band_test_1[:, :, :, np.newaxis], X_band_test_2[:, :, :, np.newaxis], X_band_test_3[:, :, :, np.newaxis]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar1 = 39\n",
    "angle_1 = train[train['inc_angle'] <= bar1]\n",
    "angle_2 = train[train['inc_angle'] > bar1]\n",
    "train_angle_split = [angle_1,angle_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = test[test['inc_angle']<=bar1].index\n",
    "index2 = test[test['inc_angle']>bar1].index\n",
    "tt = [(test1[index1],index1),(test1[index2],index2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_engineer(train):\n",
    "    \n",
    "    y_train=train['is_iceberg']\n",
    "    \n",
    "    #Generate the training data\n",
    "    X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "    X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "    X_band_3=(X_band_1+X_band_2)/2\n",
    "    X_train = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis], X_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "    return X_train,np.array(y_train),y_train.index\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get early_stopping and best_weight_save\n",
    "def get_callbacks(filepath):\n",
    "    es = EarlyStopping('val_loss', patience=13, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath,monitor='val_loss', save_best_only=True)\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=0, epsilon=1e-4, mode='min')\n",
    "    return [es, msave,reduce_lr_loss]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(horizontal_flip = True,vertical_flip = True)\n",
    "def gen_flow(X1, y,batch_size=64):\n",
    "    genX1 = datagen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            yield X1i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vgg model\n",
    "def VGG():\n",
    "    base_model = VGG16(include_top=False, input_shape=X_train.shape[1:])\n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "    \n",
    "    x = GlobalMaxPooling2D(name = \"glob_max_pool\")(x)\n",
    "    merge_one = Dense(128, activation='relu', name='fc2')(x)\n",
    "    merge_one = Dropout(0.3,name = \"drop1\")(merge_one)\n",
    "    merge_one = Dense(32, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3, name = \"drop2\")(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=base_model.input, output=predictions)   \n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(X_train,y_train,X_test,epochs = 100, k=3):\n",
    "    folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=16).split(X_train, y_train))\n",
    "    y_valid_pred = np.array([0.0]*len(X_train))\n",
    "    y_test_pred = 0\n",
    "    for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = y_train[train_idx]\n",
    "        X_valid_cv = X_train[valid_idx]\n",
    "        y_valid_cv= y_train[valid_idx]\n",
    "        \n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"%s_aug_model_weights.hdf5\"%j\n",
    "        callbacks = get_callbacks(filepath=file_path)\n",
    "        gen_fl = gen_flow(X_train_cv,y_train_cv,batch_size=64)\n",
    "        vgg16 = VGG()\n",
    "\n",
    "        vgg16.fit_generator(\n",
    "                gen_fl,\n",
    "                steps_per_epoch=12,\n",
    "                epochs=epochs,\n",
    "                shuffle=True,\n",
    "                verbose=0,\n",
    "                validation_data=(X_valid_cv, y_valid_cv),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        #Getting the Best Model\n",
    "        vgg16.load_weights(filepath=file_path)\n",
    "\n",
    "        #train predictions\n",
    "        #y_train_pred = vgg16.predict(X_train_cv)[:,0]\n",
    "        #print('Train loss: ' +str(log_loss(y_train_cv,y_train_pred)))\n",
    "\n",
    "        #validation predictions\n",
    "        y_valid_pred[valid_idx]=vgg16.predict(X_valid_cv)[:,0]\n",
    "        #print('Validation loss: ' +str(log_loss(y_valid_cv,y_valid_pred[valid_idx])))\n",
    "        \n",
    "        #Getting Test predictions\n",
    "        y_test_pred += vgg16.predict(X_test)[:,0]\n",
    "\n",
    "    print(\"Half Validation Loss: \"+str(log_loss(y_train, y_valid_pred)))\n",
    "    y_test_pred = y_test_pred.reshape((y_test_pred.shape[0]))\n",
    "    y_test_pred/=k;\n",
    "    return y_valid_pred,y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "submission = pd.DataFrame()\n",
    "submission['id']=test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zw720/pyenv/py3.5-gpu/lib/python3.5/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half Validation Loss: 0.216024247251\n",
      "Half Validation Loss: 0.307894119647\n",
      "\n",
      "Total Validation loss:0.26473838761\n",
      "\n",
      "Half Validation Loss: 0.170503736495\n",
      "Half Validation Loss: 0.269539502092\n",
      "\n",
      "Total Validation loss:0.223017602685\n",
      "\n",
      "Half Validation Loss: 0.265773233406\n",
      "Half Validation Loss: 0.322371652493\n",
      "\n",
      "Total Validation loss:0.295784631699\n",
      "\n",
      "Half Validation Loss: 0.174647203278\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "for i in range(n):\n",
    "    validation = np.array([0.0]*len(train))\n",
    "    pred = np.array([0.0]*len(test))\n",
    "    for i,angle in enumerate(train_angle_split):\n",
    "        X_train,y_train,index = data_engineer(angle)\n",
    "        X_test,index_test = tt[i]\n",
    "        validation[index],pred[index_test] = k_fold(X_train,y_train,X_test,epochs = 60)\n",
    "    \n",
    "    #loss\n",
    "    temp_loss = log_loss(train['is_iceberg'].values,validation)\n",
    "    print(\"\\nTotal Validation loss:\"+str(temp_loss)+\"\\n\")    \n",
    "    loss.append(temp_loss)\n",
    "    \n",
    "    #submission\n",
    "    submission['is_iceberg']=pred.reshape((pred.shape[0]))\n",
    "    submission.to_csv(\"/\".join(os.getcwd().split(\"/\")[:-1])+\"/Angle_Result/sub\"+str(i)+\".csv\", index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.array(loss)\n",
    "loss.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
