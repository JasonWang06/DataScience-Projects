{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "import os\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import rmsprop\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"/\".join(os.getcwd().split(\"/\")[:-1])+\"/Data/train.json\")\n",
    "test = pd.read_json(\"/\".join(os.getcwd().split(\"/\")[:-1])+\"/Data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train = train[train['inc_angle']!='na']\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "train.index = [i for i in range(len(train.index))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_band_test_3=(X_band_test_1+X_band_test_2)/2\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis], X_band_test_2[:, :, :, np.newaxis]], axis=-1)\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis], X_band_test_2[:, :, :, np.newaxis], X_band_test_3[:, :, :, np.newaxis]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar1 = 37\n",
    "bar2 = 41\n",
    "angle_1 = train[train['inc_angle'] <= bar1]\n",
    "angle_2 = train[(train['inc_angle'] > bar1) & (train['inc_angle'] <=bar2)] \n",
    "angle_3 = train[train['inc_angle'] > bar2]\n",
    "train_angle_split = [angle_1,angle_2,angle_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(angle_1))\n",
    "print(len(angle_2))\n",
    "print(len(angle_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar1 = 39\n",
    "angle_1 = train[train['inc_angle'] <= bar1]\n",
    "angle_2 = train[train['inc_angle'] > bar1]\n",
    "train_angle_split = [angle_1,angle_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691\n",
      "780\n"
     ]
    }
   ],
   "source": [
    "print(len(angle_1))\n",
    "print(len(angle_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_engineer(train):\n",
    "    \n",
    "    y_train=train['is_iceberg']\n",
    "    \n",
    "    #Generate the training data\n",
    "    X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "    X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "    X_band_3=(X_band_1+X_band_2)/2\n",
    "    X_train = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis], X_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "    return X_train,np.array(y_train),y_train.index\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get early_stopping and best_weight_save\n",
    "def get_callbacks(filepath):\n",
    "    es = EarlyStopping('val_loss', patience=13, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath,monitor='val_loss', save_best_only=True)\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')\n",
    "    return [es, msave,reduce_lr_loss]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(horizontal_flip = True,vertical_flip = True)\n",
    "def gen_flow(X1, y,batch_size=64):\n",
    "    genX1 = datagen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            yield X1i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(X_train,y_train,epochs = 100, k=3):\n",
    "    folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=16).split(X_train, y_train))\n",
    "    y_valid_pred = np.array([0.0]*len(X_train))\n",
    "    y_test_pred = 0\n",
    "    for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = y_train[train_idx]\n",
    "        X_valid_cv = X_train[valid_idx]\n",
    "        y_valid_cv= y_train[valid_idx]\n",
    "        \n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"%s_aug_model_weights.hdf5\"%j\n",
    "        callbacks = get_callbacks(filepath=file_path)\n",
    "        gen_fl = gen_flow(X_train_cv,y_train_cv,batch_size=64)\n",
    "        vgg16 = VGG()\n",
    "\n",
    "        vgg16.fit_generator(\n",
    "                gen_fl,\n",
    "                steps_per_epoch=12,\n",
    "                epochs=epochs,\n",
    "                shuffle=True,\n",
    "                verbose=0,\n",
    "                validation_data=(X_valid_cv, y_valid_cv),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        #Getting the Best Model\n",
    "        vgg16.load_weights(filepath=file_path)\n",
    "\n",
    "        #train predictions\n",
    "        y_train_pred = vgg16.predict(X_train_cv)[:,0]\n",
    "        print('Train loss: ' +str(log_loss(y_train_cv,y_train_pred)))\n",
    "\n",
    "        #validation predictions\n",
    "        y_valid_pred[valid_idx]=vgg16.predict(X_valid_cv)[:,0]\n",
    "        print('Validation loss: ' +str(log_loss(y_valid_cv,y_valid_pred[valid_idx])))\n",
    "        \n",
    "        #Getting Test predictions\n",
    "        #y_test_pred += vgg16.predict(X_test)[:,0]\n",
    "\n",
    "\n",
    "    print('\\nFinal Validation loss: ' +str(log_loss(y_train,y_valid_pred)))\n",
    "    #y_test_pred/=k;\n",
    "    return y_valid_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vgg model\n",
    "def VGG():\n",
    "    base_model = VGG16(include_top=False, input_shape=X_train.shape[1:])\n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "    \n",
    "    x = GlobalMaxPooling2D(name = \"glob_max_pool\")(x)\n",
    "    merge_one = Dense(128, activation='relu', name='fc2')(x)\n",
    "    merge_one = Dropout(0.3,name = \"drop1\")(merge_one)\n",
    "    merge_one = Dense(32, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3, name = \"drop2\")(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=base_model.input, output=predictions)   \n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zw720/pyenv/py3.5-gpu/lib/python3.5/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Train loss: 0.158845879719\n",
      "Validation loss: 0.227430136409\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python3/3.5.3/intel/lib/python3.5/site-packages/sklearn/metrics/classification.py:1662: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/share/apps/python3/3.5.3/intel/lib/python3.5/site-packages/sklearn/metrics/classification.py:1662: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: nan\n",
      "Validation loss: 0.176012634056\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Train loss: 0.170645407837\n",
      "Validation loss: 0.170548068373\n",
      "\n",
      "Final Validation loss: 0.191382522531\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Train loss: 0.326558368086\n",
      "Validation loss: 0.359847272366\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Train loss: 0.218966933156\n",
      "Validation loss: 0.235707228069\n",
      "\n",
      "Final Validation loss: 0.306916831781\n",
      "\n",
      "Total Validation loss:0.252644766729\n",
      "\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Train loss: 0.387264878908\n",
      "Validation loss: 0.40464327271\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Train loss: 0.238295993554\n",
      "Validation loss: 0.348870312726\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Train loss: nan\n",
      "Validation loss: 0.20661010948\n",
      "\n",
      "Final Validation loss: 0.320163665851\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Train loss: 0.412564226008\n",
      "Validation loss: 0.502724944666\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Train loss: 0.587615848453\n",
      "Validation loss: 0.535845765031\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Train loss: nan\n",
      "Validation loss: 0.222407770062\n",
      "\n",
      "Final Validation loss: 0.42032615992\n",
      "\n",
      "Total Validation loss:0.373274981537\n",
      "\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Train loss: 0.0907225217635\n",
      "Validation loss: 0.159166124046\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Train loss: 0.158810000329\n",
      "Validation loss: 0.200693393923\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "for i in range(n):\n",
    "    validation = np.array([0.0]*len(train))\n",
    "    for angle in train_angle_split:\n",
    "        X_train,y_train,index = data_engineer(angle)\n",
    "        validation[index] = k_fold(X_train,y_train,epochs = 100)\n",
    "    temp_loss = log_loss(train['is_iceberg'].values,validation)\n",
    "    print(\"\\nTotal Validation loss:\"+str(temp_loss)+\"\\n\")\n",
    "    loss.append(temp_loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.array(loss)\n",
    "loss.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
