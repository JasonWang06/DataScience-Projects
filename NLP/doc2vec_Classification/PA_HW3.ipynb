{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import sklearn\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2cat = {}\n",
    "t = '''C1: Airline Safety\n",
    "C2: Amphertamine\n",
    "C3: China and Spy Plan and Captives \n",
    "C4: Hoof and Mouth Desease\n",
    "C5: Iran Nuclear\n",
    "C6: Korea and Nuclear Capability \n",
    "C7: Mortrage Rates\n",
    "C8: Ocean and Pollution\n",
    "C9: Satanic Cult\n",
    "C10: Store Irene\n",
    "C11: Volcano\n",
    "C12: Saddam Hussein\n",
    "C13: Kim Jong-un\n",
    "C14: Predictive Analytics \n",
    "C15: Irma & Harvey'''.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in t:\n",
    "    temp = i.split(\":\")\n",
    "    doc2cat[temp[0]] = temp[1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my implementation of KNN, for Q1\n",
    "euc = lambda x,y:np.sqrt(np.sum((np.array(x)-np.array(y))**2)) \n",
    "def my_KNN(train,test, k):\n",
    "    pred = [];\n",
    "    for key in test:\n",
    "        case = test[key]\n",
    "        temp = {}\n",
    "        for tr in train:\n",
    "            temp[tr] = euc(case,train[tr])\n",
    "        first_k = sorted(temp, key=temp.__getitem__)[:k]\n",
    "        result = collections.defaultdict(int)\n",
    "        for doc in first_k:\n",
    "            result[doc.split('/')[0]]+=temp[doc] \n",
    "        temp_pred = sorted(result, key=result.__getitem__)[0]\n",
    "        pred.append(temp_pred)\n",
    "    \n",
    "    return pred\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sklearn's implementation of KNN, for Q3\n",
    "def KNN(train,test,k):\n",
    "    X_train = list(train.values())\n",
    "    y_train = [i.split('/')[0] for i in train.keys()]\n",
    "    X_test = list(test.values())\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(X_train, y_train) \n",
    "    return neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(train,test,k):\n",
    "    X_train = list(train.values())\n",
    "    y_train = [i.split('/')[0] for i in train.keys()]\n",
    "    X_test = list(test.values())\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "    neigh.fit(X_train, y_train) \n",
    "    return neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn's svm\n",
    "def SVM(train,test):\n",
    "    X_train = list(train.values())\n",
    "    y_train = [i.split('/')[0] for i in train.keys()]\n",
    "    X_test = list(test.values())\n",
    "    clf = svm.LinearSVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn's random forest\n",
    "def rdf(train,test):\n",
    "    X_train = list(train.values())\n",
    "    y_train = [i.split('/')[0] for i in train.keys()]\n",
    "    X_test = list(test.values())\n",
    "    clf = RandomForestClassifier(n_estimators=66,criterion='entropy', min_samples_split=3)\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf.predict(X_test)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble the above three models\n",
    "def ensemble(one,two,three):\n",
    "    ret = []\n",
    "    for index,y1 in enumerate(one):\n",
    "        pred = collections.defaultdict(int)\n",
    "        pred[two[index]]+=1;\n",
    "        pred[three[index]]+=1;\n",
    "        pred[y1]+=1;\n",
    "        ret.append(sorted(pred, key=pred.__getitem__)[0])\n",
    "        \n",
    "    return ret\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All text files are inputed\n"
     ]
    }
   ],
   "source": [
    "#tokenize txt files\n",
    "texts = []\n",
    "keys = []\n",
    "for filename in os.listdir(os.getcwd()+'/DataSet'):\n",
    "    if filename.startswith(\"C\"):\n",
    "        add = os.getcwd()+'/DataSet/'+filename;\n",
    "        for txt in os.listdir(add):\n",
    "            if txt.endswith(\"txt\"):\n",
    "                with open(add+\"//\"+txt, 'r',encoding='iso-8859-15') as myfile:\n",
    "                        key = filename+\"/\"+txt;\n",
    "                        data=myfile.read()\n",
    "                        tokens = TaggedDocument(words=nltk.word_tokenize(data), tags=[key])   \n",
    "                        texts.append(tokens)\n",
    "                        keys.append(key)\n",
    "\n",
    "print(\"All text files are inputed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params for doc2vec\n",
    "sz = 11\n",
    "wd = 6\n",
    "min_c = 2\n",
    "epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using doc2vec to convert tokenized document into vectors, this method is strictly better \n",
    "#than TF-IDF in my opinion, and is widely used in industry nowadays.\n",
    "model = doc2vec.Doc2Vec(vector_size=sz,window = wd, min_count=min_c)\n",
    "model.build_vocab(texts) \n",
    "model.train(texts, total_examples=model.corpus_count, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the best k I find for KNN was 3 \n",
    "k = 3\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=66)\n",
    "data = {}\n",
    "keys = np.array(keys)\n",
    "for key in keys:\n",
    "    data[key] = model.docvecs[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy KNN: 0.787179487179\n",
      "Accuracy SVM: 0.812820512821\n",
      "Accuracy RDF: 0.70641025641\n",
      "Accuracy ensemble: 0.712179487179\n"
     ]
    }
   ],
   "source": [
    "#K-fold to evaluate performance, for Q2\n",
    "#f1 = 0;\n",
    "acc_knn = 0;\n",
    "acc_svm = 0;\n",
    "acc_rdf = 0;\n",
    "acc_ensemble = 0\n",
    "for train_index, test_index in kf.split(keys):\n",
    "    train = {}\n",
    "    test = {}\n",
    "    for key in keys[train_index]:\n",
    "        train[key] = data[key]\n",
    "    for key in keys[test_index]:\n",
    "        test[key] = data[key]\n",
    "    y_true = [i.split('/')[0] for i in test.keys()]\n",
    "\n",
    "\n",
    "    #prediction with my KNN\n",
    "    #y_pred_my = my_KNN(train,test, k)\n",
    "    #print(f1_score(y_true,y_pred_my, average='macro'))\n",
    "    #print(accuracy_score(y_true,y_pred_my))\n",
    "\n",
    "    #prediction with sklearn KNN, svm, xgboost etc.\n",
    "    y_pred_knn = KNN(train,test,k)\n",
    "    \n",
    "    #Q5 Bonus point, apply SVM\n",
    "    y_pred_svm = SVM(train,test)\n",
    "    \n",
    "    #Q6 MODEL ensemble.\n",
    "    y_pred_rdf = rdf(train,test)\n",
    "    y_pred_ensemble = ensemble(y_pred_knn,y_pred_svm, y_pred_rdf)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #evaluate the performance using f1 score and accuracy score, for Q4\n",
    "    #actually, for multi-class classif, f1_micro = accuracy\n",
    "    #since KNN doesn't support prediction of probability, we can use multi-class cross entropy\n",
    "    #yet auc is not applicable to multi-class\n",
    "    #f1 +=f1_score(y_true,y_pred_knn, average='micro')\n",
    "    acc_knn +=accuracy_score(y_true,y_pred_knn)\n",
    "    acc_svm += accuracy_score(y_true,y_pred_svm)\n",
    "    acc_rdf += accuracy_score(y_true,y_pred_rdf)\n",
    "    acc_ensemble += accuracy_score(y_true,y_pred_ensemble)\n",
    "    \n",
    "\n",
    "#print(\"F1: \"+str(f1/10))\n",
    "print(\"Accuracy KNN: \"+str(acc_knn/10))\n",
    "print(\"Accuracy SVM: \"+str(acc_svm/10))\n",
    "print(\"Accuracy RDF: \"+str(acc_rdf/10))\n",
    "print(\"Accuracy ensemble: \"+str(acc_ensemble/10))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
