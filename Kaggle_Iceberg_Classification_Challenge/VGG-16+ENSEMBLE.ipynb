{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/share/apps/python3/3.5.3/intel/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "import os\n",
    "from matplotlib import pyplot\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import rmsprop\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping,ReduceLROnPlateau\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression as Log_Reg\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"/\".join(os.getcwd().split(\"/\")[:-1])+\"/Data/train.json\")\n",
    "test = pd.read_json(\"/\".join(os.getcwd().split(\"/\")[:-1])+\"/Data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train['is_iceberg']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "train['inc_angle']=pd.to_numeric(train['inc_angle'], errors='coerce')#We have only 133 NAs.\n",
    "train['inc_angle']=train['inc_angle'].fillna(method='pad')\n",
    "\n",
    "X_angle=train['inc_angle']\n",
    "test['inc_angle']=pd.to_numeric(test['inc_angle'], errors='coerce')\n",
    "X_test_angle=test['inc_angle']\n",
    "\n",
    "#Generate the training data\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "X_band_3=(X_band_1+X_band_2)/2\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis], X_band_3[:, :, :, np.newaxis]], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "X_band_test_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_1\"]])\n",
    "X_band_test_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test[\"band_2\"]])\n",
    "X_band_test_3=(X_band_test_1+X_band_test_2)/2\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis], X_band_test_2[:, :, :, np.newaxis]], axis=-1)\n",
    "X_test = np.concatenate([X_band_test_1[:, :, :, np.newaxis], X_band_test_2[:, :, :, np.newaxis], X_band_test_3[:, :, :, np.newaxis]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Img Generator\n",
    "datagen = ImageDataGenerator(horizontal_flip = True,vertical_flip = True)\n",
    "datagen.fit(X_train)\n",
    "def gen_flow(X1, y,batch_size=64):\n",
    "    genX1 = datagen.flow(X1,y,  batch_size=batch_size,seed=55)\n",
    "    while True:\n",
    "            X1i = genX1.next()\n",
    "            yield X1i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get early_stopping and best_weight_save\n",
    "def get_callbacks(filepath, patience=13):\n",
    "    es = EarlyStopping('val_loss', patience=patience, mode=\"min\")\n",
    "    msave = ModelCheckpoint(filepath,monitor='val_loss', save_best_only=True)\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, epsilon=1e-4, mode='min')\n",
    "    return [es, msave,reduce_lr_loss]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get vgg model\n",
    "def VGG():\n",
    "    base_model = VGG16(include_top=False, input_shape=X_train.shape[1:])\n",
    "    x = base_model.get_layer('block5_pool').output\n",
    "    \n",
    "    x = GlobalMaxPooling2D(name = \"glob_max_pool\")(x)\n",
    "    merge_one = Dense(128, activation='relu', name='fc2')(x)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    merge_one = Dense(32, activation='relu', name='fc3')(merge_one)\n",
    "    merge_one = Dropout(0.3)(merge_one)\n",
    "    \n",
    "    predictions = Dense(1, activation='sigmoid')(merge_one)\n",
    "    \n",
    "    model = Model(input=base_model.input, output=predictions)   \n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(vgg16_feature,train_idx,valid_idx,y_train, y_valid, X_test,train,valid,test):\n",
    "    X_train = vgg16_feature[train_idx]\n",
    "    X_valid = vgg16_feature[valid_idx]\n",
    "    \n",
    "    \n",
    "    #XGboost\n",
    "    XGB = xgb.XGBClassifier(reg_lambda = 1.6, max_depth=3, n_estimators=300, learning_rate=0.05,objective=\"binary:logistic\").fit(X_train, y_train)\n",
    "    train[\"Xgbosst\"] = XGB.predict_proba(X_train)[:,1]\n",
    "    valid.loc[valid_idx,\"Xgbosst\"] = XGB.predict_proba(X_valid)[:,1]\n",
    "    test[\"Xgbosst\"] = XGB.predict_proba(X_test)[:,1]\n",
    "    print(\"XGboost train loss: \"+str(log_loss(y_train,train[\"Xgbosst\"])))\n",
    "    print(\"XGboost validation loss: \"+str(log_loss(y_valid,valid.loc[valid_idx,\"Xgbosst\"])))\n",
    "    print()\n",
    " \n",
    "    \n",
    "    #svm\n",
    "    svm_clf = svm.SVC(C= 0.6, probability = True).fit(X_train, y_train) \n",
    "    train[\"SVM\"] = svm_clf.predict_proba(X_train)[:,1]\n",
    "    valid.loc[valid_idx,\"SVM\"] = svm_clf.predict_proba(X_valid)[:,1]\n",
    "    test[\"SVM\"] = svm_clf.predict_proba(X_test)[:,1]\n",
    "    print(\"SVM train loss: \"+str(log_loss(y_train,train[\"SVM\"])))\n",
    "    print(\"SVM validation loss: \"+str(log_loss(y_valid,valid.loc[valid_idx,\"SVM\"])))\n",
    "    print()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    #RandomForestClassifier\n",
    "    rf_clf = RandomForestClassifier(max_depth=3).fit(X_train, y_train)\n",
    "    train[\"RandForest\"] = rf_clf.predict_proba(X_train)[:,1]\n",
    "    valid.loc[valid_idx,\"RandForest\"] = rf_clf.predict_proba(X_valid)[:,1]\n",
    "    test[\"RandForest\"] = rf_clf.predict_proba(X_test)[:,1]\n",
    "    print(\"RandomForestClassifier train loss: \"+str(log_loss(y_train,train[\"RandForest\"])))\n",
    "    print(\"RandomForestClassifier validation loss: \"+str(log_loss(y_valid,valid.loc[valid_idx,\"RandForest\"])))\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return [train,valid,test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = list(StratifiedKFold(n_splits=3, shuffle=True, random_state=16).split(X_train, y_train))\n",
    "valid = pd.DataFrame(index = [i for i in range(X_train.shape[0])])\n",
    "test = pd.DataFrame()\n",
    "log_reg = np.array([0.1]*len(X_train))\n",
    "\n",
    "for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "    train = pd.DataFrame()\n",
    "    X_train_cv = X_train[train_idx]\n",
    "    y_train_cv = y_train[train_idx]\n",
    "    X_valid_cv = X_train[valid_idx]\n",
    "    y_valid_cv= y_train[valid_idx]\n",
    "\n",
    "\n",
    "    #define file path and get callbacks\n",
    "    file_path = \"%s_aug_model_weights.hdf5\"%j\n",
    "    callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "    gen_fl = gen_flow(X_train_cv,y_train_cv)\n",
    "    vgg16 = VGG()\n",
    "\n",
    "    vgg16.fit_generator(\n",
    "            gen_fl,\n",
    "            steps_per_epoch=24,\n",
    "            epochs=100,\n",
    "            shuffle=True,\n",
    "            verbose=1,\n",
    "            validation_data=(X_valid_cv, y_valid_cv),\n",
    "            callbacks=callbacks)\n",
    "\n",
    "    #Getting the Best Model\n",
    "    vgg16.load_weights(filepath=file_path)\n",
    "\n",
    "    #train predictions\n",
    "    y_train_pred = vgg16.predict(X_train_cv)\n",
    "    train['Vgg16'] = y_train_pred.reshape(y_train_pred.shape[0])\n",
    "\n",
    "    #validation predictions\n",
    "    y_valid_pred=vgg16.predict(X_valid_cv)\n",
    "    valid.loc[valid_idx,'Vgg16'] = y_valid_pred.reshape(y_valid_pred.shape[0])\n",
    "\n",
    "    #Getting Test predictions\n",
    "    y_test_pred = vgg16.predict(X_test)\n",
    "    test['Vgg16'] = y_test_pred.reshape(y_test_pred.shape[0])\n",
    "\n",
    "    #ensemble\n",
    "    intermediate_layer_model = Model(inputs=vgg16.input,outputs=vgg16.get_layer('glob_max_pool').output)\n",
    "    vgg16_feature = intermediate_layer_model.predict(X_train);\n",
    "    vgg16_test = intermediate_layer_model.predict(X_test)\n",
    "    train,valid,test = ensemble(vgg16_feature,train_idx,valid_idx,y_train_cv, y_valid_cv, vgg16_test,train,valid,test);\n",
    "\n",
    "    #logistic regression to assign weights\n",
    "    logistic = Log_Reg(C = 0.6).fit(train,y_train_cv)\n",
    "    log_train = logistic.predict_proba(train)\n",
    "    log_reg[valid_idx] = logistic.predict_proba(valid.loc[valid_idx])[:,1]\n",
    "    logistic_test = logistic.predict_proba(test)[:,1];\n",
    "\n",
    "    print('Logistic Regression train loss: ' +str(log_loss(y_train_cv,log_train)))\n",
    "    print('Logistic Regression validation loss: ' +str(log_loss(y_valid_cv,log_reg[valid_idx])))\n",
    "    print()\n",
    "\n",
    "    '''print(\"\\ntrain score:\")\n",
    "    for clf in train.columns:\n",
    "        print(clf+\": \"+str(log_loss(y_train_cv,train[clf])))\n",
    "    print(\"Logistic_Reg: \"+str(log_loss(y_train_cv,logistic_train)))'''\n",
    "    break\n",
    "\n",
    "\n",
    "print(\"\\nValidation Results:\\n\")\n",
    "valid['Logistic_Regression'] = log_reg\n",
    "for clf in valid:\n",
    "    print(clf+\": \"+str(log_loss(y_train,valid[clf])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myAngleCV(X_train, X_test,y_train,K=3):\n",
    "    folds = list(StratifiedKFold(n_splits=K, shuffle=True, random_state=16).split(X_train, y_train))\n",
    "    valid = pd.DataFrame(index = [i for i in range(X_train.shape[0])])\n",
    "    test = pd.DataFrame()\n",
    "    log_reg = np.array([0]*len(X_train))\n",
    "    \n",
    "    for j, (train_idx, valid_idx) in enumerate(folds):\n",
    "        train = pd.DataFrame()\n",
    "        X_train_cv = X_train[train_idx]\n",
    "        y_train_cv = y_train[train_idx]\n",
    "        X_valid_cv = X_train[valid_idx]\n",
    "        y_valid_cv= y_train[valid_idx]\n",
    "        \n",
    "\n",
    "        #define file path and get callbacks\n",
    "        file_path = \"%s_aug_model_weights.hdf5\"%j\n",
    "        callbacks = get_callbacks(filepath=file_path, patience=5)\n",
    "        gen_fl = gen_flow(X_train_cv,y_train_cv)\n",
    "        vgg16 = VGG()\n",
    "        \n",
    "        vgg16.fit_generator(\n",
    "                gen_fl,\n",
    "                steps_per_epoch=24,\n",
    "                epochs=100,\n",
    "                shuffle=True,\n",
    "                verbose=1,\n",
    "                validation_data=(X_valid_cv, y_valid_cv),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "        #Getting the Best Model\n",
    "        vgg16.load_weights(filepath=file_path)\n",
    "\n",
    "        #train predictions\n",
    "        y_train_pred = vgg16.predict(X_train_cv)\n",
    "        train['Vgg16'] = y_train_pred.reshape(y_train_pred.shape[0])\n",
    "        \n",
    "        #validation predictions\n",
    "        y_valid_pred=vgg16.predict(X_valid_cv)\n",
    "        valid.loc[valid_idx,'Vgg16'] = y_valid_pred.reshape(y_valid_pred.shape[0])\n",
    "\n",
    "        #Getting Test predictions\n",
    "        y_test_pred = vgg16.predict(X_test)\n",
    "        test['Vgg16'] = y_test_pred.reshape(y_test_pred.shape[0])\n",
    "        \n",
    "        #ensemble\n",
    "        intermediate_layer_model = Model(inputs=vgg16.input,outputs=vgg16.get_layer('glob_max_pool').output)\n",
    "        vgg16_feature = intermediate_layer_model.predict(X_train);\n",
    "        vgg16_test = intermediate_layer_model.predict(X_test)\n",
    "        train,valid,test = ensemble(vgg16_feature,train_idx,valid_idx,y_train_cv, y_valid_cv, vgg16_test,train,valid,test);\n",
    "        \n",
    "        #logistic regression to assign weights\n",
    "        logistic = Log_Reg().fit(train,y_train_cv)\n",
    "        logistic_train =  logistic.predict_proba(train)[:,1];\n",
    "        log_reg[[valid_idx]] = logistic.predict_proba(valid.loc[valid_idx])[:,1];\n",
    "        logistic_test = logistic.predict_proba(test)[:,1];\n",
    "        for clf in train.columns:\n",
    "            print(clf+\": \"+str(log_loss(y_train_cv,train[clf])))\n",
    "        print(\"Logistic_Reg: \"+str(log_loss(y_train_cv,logistic_train)))\n",
    "\n",
    "    print(\"\\n Validation Results\\n:\")\n",
    "    valid['Logistic_Regression'] = log_reg\n",
    "    for clf in valid:\n",
    "        print(clf+\": \"+str(log_loss(y_train,valid[clf])))\n",
    "    \n",
    "    return valid,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for i in range(n):\n",
    "    valid,submit=myAngleCV(X_train,X_test,y_train,K=3)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
